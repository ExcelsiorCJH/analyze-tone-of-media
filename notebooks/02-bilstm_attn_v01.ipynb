{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM + Attention Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import dill\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from functools import partial\n",
    "from collections import Counter, defaultdict\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from utils.types_ import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/tokenized/nouns_total_data.txt\"\n",
    "with open(data_path, \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword, press, category, title, content\n",
    "# columns = [\"keyword\", \"press\", \"category\", \"title\", \"content\"]\n",
    "# df = pd.DataFrame(data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_len = [len(d[4]) for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237.42394667936205"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(text_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(\n",
    "    dataset: List[Tuple], save_dir: str, num_words: int = 30000\n",
    ") -> Union[Dict, Dict]:\n",
    "    # 1. tokenization\n",
    "    all_tokens = []\n",
    "    for data in tqdm(dataset):\n",
    "        all_tokens.extend(data[4])\n",
    "\n",
    "    # 2. build vocab\n",
    "    vocab = Counter(all_tokens)\n",
    "    vocab = vocab.most_common(num_words)\n",
    "\n",
    "    # 3. add pad & unk tokens\n",
    "    word_index = defaultdict()\n",
    "    word_index[\"<PAD>\"] = 0\n",
    "    word_index[\"<UNK>\"] = 1\n",
    "\n",
    "    for idx, (word, _) in enumerate(vocab, 2):\n",
    "        word_index[word] = idx\n",
    "\n",
    "    index_word = {idx: word for word, idx in word_index.items()}\n",
    "\n",
    "    with open(f\"{save_dir}/word_index.pkl\", \"wb\") as f:\n",
    "        dill.dump(word_index, f)\n",
    "\n",
    "    return word_index, index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16804/16804 [00:00<00:00, 412672.05it/s]\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"../data/vocab\"\n",
    "word_index, index_word = build_vocab(data, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Create Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NewsDataset(Dataset):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         path: str,\n",
    "#         word_index: Dict,\n",
    "#         max_len: int = 256,\n",
    "#         labels: List = [\"조선일보\", \"동아일보\", \"경향신문\", \"한겨레\"],\n",
    "#     ):\n",
    "#         self.word_index = word_index\n",
    "#         self.max_len = max_len\n",
    "#         self.label_dict = {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "#         with open(path, \"rb\") as f:\n",
    "#             self.data = pickle.load(f)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         keyword = self.data[idx][0]\n",
    "#         label = self.data[idx][1]\n",
    "#         label = self.label_dict[label]\n",
    "#         text = self.data[idx][4]\n",
    "#         sequence = self.text_to_sequence(text)\n",
    "#         sequence = self.pad_sequence(sequence)\n",
    "#         return sequence, label, keyword\n",
    "\n",
    "#     def text_to_sequence(self, text):\n",
    "#         sequence = [self.word_index.get(word, 1) for word in text]\n",
    "#         return sequence\n",
    "\n",
    "#     def pad_sequence(self, sequence):\n",
    "#         if len(sequence) > self.max_len:\n",
    "#             sequence = sequence[: self.max_len]\n",
    "#         else:\n",
    "#             sequence = [0 for _ in range(self.max_len - len(sequence))] + sequence\n",
    "#         return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, path: str):\n",
    "        with open(path, \"rb\") as f:\n",
    "            self.data = pickle.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        keyword = self.data[idx][0]\n",
    "        label = self.data[idx][1]\n",
    "        text = self.data[idx][4]\n",
    "        return text, label, keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/tokenized/nouns_total_data.txt\"\n",
    "dataset = NewsDataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(dataset[200][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2) DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, word_index, labels_dict, max_len=200):\n",
    "    texts = [entry[0] for entry in batch]\n",
    "    labels = [entry[1] for entry in batch]\n",
    "    keywords = [entry[2] for entry in batch]\n",
    "\n",
    "    sequences = []  # [[word_index.get(word, 1) for word in text] for text in texts]\n",
    "    for text in texts:\n",
    "        if len(text) > max_len:\n",
    "            sequence = [word_index.get(word, 1) for word in text[:max_len]]\n",
    "        else:\n",
    "            sequence = [0 for _ in range(max_len - len(text))] + [\n",
    "                word_index.get(word, 1) for word in text\n",
    "            ]\n",
    "\n",
    "        sequences.append(sequence)\n",
    "\n",
    "    labels = [labels_dict[label] for label in labels]\n",
    "    return sequences, labels, keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = [\"조선일보\", \"동아일보\", \"경향신문\", \"한겨레\"]\n",
    "labels_dict = {label: idx for idx, label in enumerate(labels_list)}\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=partial(collate_fn, word_index=word_index, labels_dict=labels_dict),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in dataloader:\n",
    "#     sequences, labels, keywords = batch\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiLSTM + Attention Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "#         super(Attention, self).__init__()\n",
    "\n",
    "#         self.attn = nn.Linear(enc_hid_dim + dec_hid_dim, dec_hid_dim)\n",
    "#         self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "\n",
    "#     def forward(self, hidden, encoder_outputs):\n",
    "#         batch_size = encoder_outputs.size(0)\n",
    "#         src_len = encoder_outputs.size(1)\n",
    "\n",
    "#         hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "\n",
    "#         score = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "#         attention = self.v(score).squeeze(2)\n",
    "\n",
    "#         return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) BiLSTM + Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMAttn(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        num_class: int,\n",
    "        embed_dim: int = 128,\n",
    "        hidden_dim: int = 256,\n",
    "        num_layers: int = 2,\n",
    "        bidirectional: bool = True,\n",
    "        dropout_p: float = 0.3,\n",
    "    ):\n",
    "        super(BiLSTMAttn, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directs = 1\n",
    "        if bidirectional:\n",
    "            self.num_directs = 2\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        nn.init.xavier_uniform_(self.embed.weight)\n",
    "        self.bilstm = nn.LSTM(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_p,\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_dim * 2, num_class)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def attention_layer(self, query, key):\n",
    "        query = query.squeeze(0)\n",
    "        attn_scores = torch.bmm(key, query.unsqueeze(2)).squeeze(2)\n",
    "        attn_weights = F.softmax(attn_scores, dim=1)\n",
    "        contexts = torch.bmm(key.transpose(1, 2), attn_weights.unsqueeze(2)).squeeze(2)\n",
    "        return contexts, attn_weights\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        x = self.embed(sequence)\n",
    "        x = self.dropout(x)\n",
    "        output, _ = self.bilstm(x)\n",
    "        output, attn_weights = self.attention_layer(output[:, -1, :], output)\n",
    "        output = self.linear(output)\n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "GPU_NUM = 1\n",
    "DEVICE = torch.device(f\"cuda:{GPU_NUM}\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_index)\n",
    "num_class = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTMAttn(vocab_size, num_class).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    sequences, labels, keywords = batch\n",
    "    break\n",
    "\n",
    "sequences = torch.LongTensor(sequences).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0396, 0.0111, 0.0128, 0.0278],\n",
       "         [0.0387, 0.0108, 0.0126, 0.0280]], device='cuda:1',\n",
       "        grad_fn=<AddmmBackward>),\n",
       " tensor([[0.0045, 0.0047, 0.0048, 0.0049, 0.0049, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0049, 0.0048],\n",
       "         [0.0045, 0.0047, 0.0048, 0.0049, 0.0049, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0050,\n",
       "          0.0049, 0.0048]], device='cuda:1', grad_fn=<SoftmaxBackward>))"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
