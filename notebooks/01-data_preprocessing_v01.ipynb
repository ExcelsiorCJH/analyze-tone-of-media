{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import pickle\n",
    "import platform  # os check\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import get_tokenizer\n",
    "from utils.types_ import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Load\n",
    "\n",
    "- 전체 데이터 한번에 묶어서 처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 37.50it/s]\n"
     ]
    }
   ],
   "source": [
    "data_paths = glob.glob(\"../data/origins/*/*.txt\")\n",
    "data_paths = sorted(data_paths)\n",
    "data_dict = defaultdict(list)\n",
    "\n",
    "for data_path in tqdm(data_paths):\n",
    "    keyword = data_path.split(\"/\")[-2]\n",
    "    with open(data_path, \"rb\") as fp:\n",
    "        data_dict[keyword].extend(pickle.load(fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dict['탄핵'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_merge(dir_path: str):\n",
    "    data_paths = glob.glob(f\"{dir_path}/*/*.txt\")\n",
    "    data_paths = sorted(data_paths)\n",
    "    data_dict = defaultdict(list)\n",
    "\n",
    "    for data_path in tqdm(data_paths):\n",
    "        keyword = data_path.split(\"/\")[-2]\n",
    "        with open(data_path, \"rb\") as fp:\n",
    "            data_dict[keyword].extend(pickle.load(fp))\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 36.03it/s]\n"
     ]
    }
   ],
   "source": [
    "dir_path = \"../data/origins\"\n",
    "\n",
    "data_dict = data_merge(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "- 제목, 기사에 대하여 불필요한 텍스트 정제 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"기사 내용 전처리 함수\n",
    "    Args:\n",
    "        - text: str 형태의 텍스트\n",
    "    Return:\n",
    "        - text: 전처리된 텍스트\"\"\"\n",
    "    # Common\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    # E-mail 제거#\n",
    "    text = re.sub(\"([\\w\\d.]+@[\\w\\d.]+)\", \"\", text)\n",
    "    text = re.sub(\"([\\w\\d.]+@)\", \"\", text)\n",
    "    # 괄호 안 제거#\n",
    "    text = re.sub(\"<[\\w\\s\\d‘’=/·~:&,`]+>\", \"\", text)\n",
    "    text = re.sub(\"\\([\\w\\s\\d‘’=/·~:&,`]+\\)\", \"\", text)\n",
    "    text = re.sub(\"\\[[\\w\\s\\d‘’=/·~:&,`]+\\]\", \"\", text)\n",
    "    text = re.sub(\"【[\\w\\s\\d‘’=/·~:&,`]+】\", \"\", text)\n",
    "    text = re.sub(\"\\(.*\\)\", \"\", text)\n",
    "    text = re.sub(\"\\[.*\\]\", \"\", text)\n",
    "\n",
    "    # 전화번호 제거#\n",
    "    text = re.sub(\"(\\d{2,3})-(\\d{3,4}-\\d{4})\", \"\", text)  # 전화번호\n",
    "    text = re.sub(\"(\\d{3,4}-\\d{4})\", \"\", text)  # 전화번호\n",
    "    # 홈페이지 주소 제거#\n",
    "    text = re.sub(\"(https:)\", \"\", text)\n",
    "    text = re.sub(\"(www.\\w.+)\", \"\", text)\n",
    "    text = re.sub(\"(.\\w+.com)\", \"\", text)\n",
    "    text = re.sub(\"(.\\w+.co.kr)\", \"\", text)\n",
    "    text = re.sub(\"(.\\w+.go.kr)\", \"\", text)\n",
    "    # 기자 이름 제거#\n",
    "    text = re.sub(\"/\\w+[=·\\w@]+\\w+\\s[=·\\w@]+\", \"\", text)\n",
    "    text = re.sub(\"\\w{2,4}\\s기자\", \"\", text)\n",
    "    # 한자 제거#\n",
    "    text = re.sub(\"[\\u2E80-\\u2EFF\\u3400-\\u4DBF\\u4E00-\\u9FBF\\uF900]+\", \"\", text)\n",
    "    # 특수기호 제거#\n",
    "    text = re.sub(\"[◇#/▶▲◆■●△①②③★○◎▽=▷☞◀ⓒ□?㈜♠☎]\", \"\", text)\n",
    "    # 따옴표 제거#\n",
    "    text = re.sub(\"[\\\"'”“‘’]\", \"\", text)\n",
    "    # 2안_숫자제거#\n",
    "    # text = regex.sub('[0-9]+',\"\",text)\n",
    "\n",
    "    text = \" \".join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8018/8018 [00:03<00:00, 2475.75it/s]\n",
      "100%|██████████| 3343/3343 [00:01<00:00, 3058.51it/s]\n",
      "100%|██████████| 1756/1756 [00:00<00:00, 2977.08it/s]\n",
      "100%|██████████| 9007/9007 [00:04<00:00, 2206.66it/s]\n",
      "100%|██████████| 1392/1392 [00:00<00:00, 2542.62it/s]\n"
     ]
    }
   ],
   "source": [
    "clean_data = []\n",
    "for key, articles in data_dict.items():\n",
    "    for article in tqdm(articles):\n",
    "        press, cat, title, content = article\n",
    "        if press in [\"조선일보\", \"동아일보\", \"경향신문\", \"한겨레\"]:\n",
    "            title = clean_text(title)\n",
    "            content = clean_text(content)\n",
    "\n",
    "            clean_data.append((key, press, cat, title, content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/total_cleaned_data.txt', 'wb') as fp:\n",
    "#     pickle.dump(clean_data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clean(data_dict: Dict, press_list: List[str]) -> List[Tuple]:\n",
    "    clean_data = []\n",
    "    for key, articles in data_dict.items():\n",
    "        for article in tqdm(articles, desc=f\"{key} data_clean\"):\n",
    "            press, cat, title, content = article\n",
    "            if press in press_list:\n",
    "                title = clean_text(title)\n",
    "                content = clean_text(content)\n",
    "\n",
    "                clean_data.append((key, press, cat, title, content))\n",
    "\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "남북회담 data_clean: 100%|██████████| 8018/8018 [00:03<00:00, 2540.31it/s]\n",
      "드루킹 data_clean: 100%|██████████| 3343/3343 [00:01<00:00, 3004.17it/s]\n",
      "조국 data_clean: 100%|██████████| 1756/1756 [00:00<00:00, 2867.22it/s]\n",
      "탄핵 data_clean: 100%|██████████| 9007/9007 [00:04<00:00, 2201.69it/s]\n",
      "필리버스터 data_clean: 100%|██████████| 1392/1392 [00:00<00:00, 2552.92it/s]\n"
     ]
    }
   ],
   "source": [
    "press_list = [\"조선일보\", \"동아일보\", \"경향신문\", \"한겨레\"]\n",
    "clean_data = data_clean(data_dict, press_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(\n",
    "    data: List[Tuple],\n",
    "    save_dir: str,\n",
    "    stopwords_path: str,\n",
    "    tokenizer_name: str = \"mecab\",\n",
    ") -> None:\n",
    "\n",
    "    with open(stopwords_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        stopwords = f.read().split(\"\\n\")\n",
    "\n",
    "    tokenizer = get_tokenizer(tokenizer_name)\n",
    "    nouns_data, token_data, token_pos_data = [], [], []\n",
    "    for news in tqdm(data):\n",
    "        keyword, press, category, title, content = news\n",
    "\n",
    "        # tokenizer를 이용한 tokenizing\n",
    "        # nouns\n",
    "        title_nouns = tokenizer.nouns(title)\n",
    "        content_nouns = tokenizer.nouns(content)\n",
    "        # tokens & pos_tag\n",
    "        title_tokens = tokenizer.pos(title)\n",
    "        content_tokens = tokenizer.pos(content)\n",
    "\n",
    "        # stopwords 적용\n",
    "        title_nouns = [word for word in title_nouns if word not in stopwords]\n",
    "        content_nouns = [word for word in content_nouns if word not in stopwords]\n",
    "        title_morphs = [word for word, _ in title_tokens if word not in stopwords]\n",
    "        content_morphs = [word for word, _ in content_tokens if word not in stopwords]\n",
    "        title_tags = [\n",
    "            f\"{word}_{pos}\" for word, pos in title_tokens if word not in stopwords\n",
    "        ]\n",
    "        content_tags = [\n",
    "            f\"{word}_{pos}\" for word, pos in content_tokens if word not in stopwords\n",
    "        ]\n",
    "\n",
    "        # append lists\n",
    "        nouns_data.append((keyword, press, category, title_nouns, content_nouns))\n",
    "        token_data.append((keyword, press, category, title_morphs, content_morphs))\n",
    "        token_pos_data.append((keyword, press, category, title_tags, content_tags))\n",
    "\n",
    "    # save tokens\n",
    "    with open(f\"{save_dir}/nouns_total_data.txt\", \"wb\") as fp:\n",
    "        pickle.dump(nouns_data, fp)\n",
    "\n",
    "    with open(f\"{save_dir}/tokenized_pos_total_data.txt\", \"wb\") as fp:\n",
    "        pickle.dump(token_pos_data, fp)\n",
    "\n",
    "    with open(f\"{save_dir}/tokenized_total_data.txt\", \"wb\") as fp:\n",
    "        pickle.dump(token_data, fp)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16804/16804 [03:55<00:00, 71.44it/s] \n"
     ]
    }
   ],
   "source": [
    "save_dir = \"../data/tokenized\"\n",
    "stopwords_path = \"../data/stopwords/stopwords_kr.txt\"\n",
    "get_tokens(clean_data, save_dir, stopwords_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
